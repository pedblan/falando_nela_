import sqlite3
import numpy as np
import pandas as pd
import faiss
from sklearn.feature_extraction.text import CountVectorizer
from umap import UMAP
from hdbscan import HDBSCAN
from bertopic.representation import PartOfSpeech, MaximalMarginalRelevance
from sentence_transformers import SentenceTransformer
from bertopic import BERTopic
import os
import openai
client = openai.OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
from typing import Tuple, List, Optional, Dict, Any, Union
from io import StringIO

CAMINHO_BANCO = "../../DiscursosSenadores_02_05_2025_analisado.sqlite"


def carregar_dados() -> pd.DataFrame:
    """
    Carrega os dados dos discursos do Senado a partir do banco de dados SQLite.

    A fun√ß√£o realiza uma jun√ß√£o entre as tabelas AnaliseCorpusTodo, Discursos e Senadores,
    filtrando apenas os discursos da Casa Legislativa 'SF' (Senado Federal).

    Returns:
        pd.DataFrame: Um DataFrame contendo os discursos e metadados associados.
    """
    conexao = sqlite3.connect(CAMINHO_BANCO)

    consulta = """
    SELECT
        a.*,
        d.TextoResumo,
        d.Indexacao,
        d.DataPronunciamento,
        d.SiglaCasaPronunciamento,
        s.NomeParlamentar,
        s.CodigoParlamentar,
        d.SiglaPartidoParlamentarNaData,
        d.UfParlamentarNaData
    FROM AnaliseCorpusTodo AS a
    LEFT JOIN Discursos AS d ON a.CodigoPronunciamento = d.CodigoPronunciamento
    LEFT JOIN Senadores AS s ON d.CodigoParlamentar = s.CodigoParlamentar
    WHERE d.SiglaCasaPronunciamento = 'SF'
    """

    df_discursos = pd.read_sql_query(consulta, conexao)
    conexao.close()

    return df_discursos


def carregar_embeddings(coluna: str) -> Tuple[np.ndarray, np.ndarray]:
    """
    Carrega os embeddings e os c√≥digos de documentos a partir de arquivos FAISS e NumPy.

    Os arquivos s√£o carregados com base no nome da coluna especificada, que define o sufixo
    dos arquivos de embedding.

    Args:
        coluna (str): Nome da coluna usada para identificar os arquivos de embedding.

    Returns:
        Tuple[np.ndarray, np.ndarray]: Um par contendo os c√≥digos dos documentos (IDs) e os embeddings correspondentes.
    """
    caminho_index = f"../../../data/discursos/embeddings/discursos_{coluna}.index"
    caminho_codigos = f"../../../data/discursos/embeddings/codigos_{coluna}.npy"

    codigos = np.load(caminho_codigos)
    faiss_index = faiss.read_index(caminho_index)
    embeddings = faiss_index.reconstruct_n(0, faiss_index.ntotal)

    return codigos, embeddings

def verificar_embeddings(
    codigos: np.ndarray,
    embeddings: np.ndarray,
    df_discursos: pd.DataFrame
) -> Tuple[pd.DataFrame, np.ndarray]:
    """
    Alinha os embeddings com os dados do DataFrame de discursos com base no C√≥digoPronunciamento.

    Cria um DataFrame auxiliar com os c√≥digos dos embeddings e seus √≠ndices, e realiza a jun√ß√£o
    com o DataFrame dos discursos. Em seguida, seleciona apenas os embeddings v√°lidos e alinhados.

    Args:
        codigos (np.ndarray): Array com os c√≥digos dos pronunciamentos (IDs dos documentos).
        embeddings (np.ndarray): Matriz de embeddings correspondente aos c√≥digos.
        df_discursos (pd.DataFrame): DataFrame contendo os discursos e metadados.

    Returns:
        Tuple[pd.DataFrame, np.ndarray]:
            - DataFrame com os discursos v√°lidos e alinhados com os embeddings.
            - Matriz de embeddings alinhada com o DataFrame.
    """
    print("Shape dos embeddings:", embeddings.shape)
    print("Quantidade de c√≥digos:", codigos.shape)

    df_embeddings = pd.DataFrame({
        "CodigoPronunciamento": codigos,
        "embedding_index": np.arange(len(codigos))
    })

    df_final_valido = pd.merge(
        df_discursos,
        df_embeddings,
        on="CodigoPronunciamento",
        how="inner"
    )

    embeddings_validos = embeddings[df_final_valido["embedding_index"].values]

    print(f"N¬∫ de documentos v√°lidos: {len(df_final_valido)}")
    print(f"Shape dos embeddings v√°lidos: {embeddings_validos.shape}")

    return df_final_valido, embeddings_validos


def formatar_datas(df: pd.DataFrame) -> pd.DataFrame:
    """
    Converte a coluna 'DataPronunciamento' para formato datetime e filtra os anos desejados.

    A fun√ß√£o tamb√©m reporta quantas datas n√£o puderam ser convertidas.
    Se o par√¢metro anos n√£o for ['todos'], a fun√ß√£o filtra os discursos pelos anos informados.

    Args:
        df (pd.DataFrame): DataFrame contendo a coluna 'DataPronunciamento'.
        anos (List[int]): Lista de anos a manter no DataFrame ou ['todos'] para n√£o filtrar.

    Returns:
        pd.DataFrame: DataFrame com datas tratadas e possivelmente filtrado por ano.
    """
    df["DataPronunciamento"] = pd.to_datetime(df["DataPronunciamento"], errors="coerce")
    problemas = df["DataPronunciamento"].isnull().sum()
    print(f"Problemas de datas: {problemas}")

    return df

def salvar_dataset_embeddings(
    df: pd.DataFrame,
    embeddings: np.ndarray,
    coluna: str
) -> None:
    """
    Salva o DataFrame com os discursos alinhados e os embeddings correspondentes em disco.

    O DataFrame √© salvo no formato Parquet, e os embeddings s√£o salvos como arquivo .npy.

    Args:
        df (pd.DataFrame): DataFrame com os discursos e metadados j√° alinhados aos embeddings.
        embeddings (np.ndarray): Matriz de embeddings correspondente.
        coluna (str): Nome da coluna usada como sufixo nos nomes dos arquivos salvos.

    Returns:
        None
    """
    DIRETORIO_DATASET = f"./{coluna}"
    if not os.path.exists(DIRETORIO_DATASET):
        os.makedirs(DIRETORIO_DATASET)

    caminho_parquet = f"{DIRETORIO_DATASET}/dataset_preparado_{coluna}.parquet"
    caminho_npy = f"{DIRETORIO_DATASET}/embeddings_validos_{coluna}.npy"

    df.to_parquet(caminho_parquet, index=False)
    np.save(caminho_npy, embeddings)

    print(f"Arquivos salvos: {caminho_parquet} e {caminho_npy}")


def preparar_docs_bertopic(df: pd.DataFrame, coluna: str) -> List[str]:
    """
    Extrai e valida os documentos de uma coluna espec√≠fica do DataFrame para uso no BERTopic.

    Substitui entradas inv√°lidas ou vazias por um marcador neutro ('texto_vazio').

    Args:
        df (pd.DataFrame): DataFrame contendo a coluna de texto a ser analisada.
        coluna (str): Nome da coluna que cont√©m os textos.

    Returns:
        List[str]: Lista de textos validados e prontos para modelagem.
    """
    docs = df[coluna].tolist()
    print(f"Total de documentos extra√≠dos: {len(docs)}")

    docs_validados = [
        doc if isinstance(doc, str) and doc.strip() else "texto_vazio"
        for doc in docs
    ]

    return docs_validados


def treinar_vetorizador() -> CountVectorizer:
    """
    Cria e retorna um modelo de vetoriza√ß√£o com CountVectorizer adaptado para textos em portugu√™s.

    O modelo utiliza n-gramas de 1 a 4 palavras e uma lista personalizada de stopwords
    para eliminar ru√≠dos comuns em discursos parlamentares.

    Returns:
        CountVectorizer: Inst√¢ncia configurada do vetorizador.
    """
    stopwords_extras = [
        "texto_vazio", "textovazio", "nenhuma", "nenhum", "null", "n√£o aplic√°vel",
        "resposta n√£o informada", "Por causa disso, o orador", "Por", "causa", "disso",
        "o", "os", "as", "orador", "de", "que", "para", "do", "em", "ao", "dos", "das",
        "da", "seu", "sua", "por", "no", "na", "um", "uma", "pelo", "pela", "aos", "√†s",
        "isso significa que o", "isso significa que a", "√©", "s√£o",
        "constitui√ß√£o de", "constitui√ß√£o do", "constitui√ß√£o da",
        "suas", "seus", "com", "como", "ser", "se", "isso", "Isso", "Eu", "eu",
        "sobre", "conforme previsto", "conforme estabelecido", "Constitui√ß√£o Federal", "Constitui√ß√£o", "art", "arts"
    ]

    vectorizer_model = CountVectorizer(
        stop_words=stopwords_extras,
        strip_accents=None,
        lowercase=False,
        token_pattern=r'(?u)\b[A-Za-z√Ä-√ñ√ò-√∂√∏-√ø]{2,}\b',
        ngram_range=(1, 4),
        min_df=10
    )

    return vectorizer_model


def reduzir_dim_embeddings(embeddings: np.ndarray) -> np.ndarray:
    """
    Reduz a dimensionalidade dos embeddings utilizando UMAP.

    A redu√ß√£o √© feita para 100 dimens√µes com m√©trica de cosseno, mantendo vizinhan√ßas
    locais. Isso √© √∫til para clustering com HDBSCAN e visualiza√ß√µes posteriores.

    Args:
        embeddings (np.ndarray): Matriz de embeddings original (alta dimensionalidade).

    Returns:
        np.ndarray: Matriz de embeddings com dimensionalidade reduzida.
    """
    umap_model = UMAP(
        n_neighbors=15,
        n_components=100,
        min_dist=0.0,
        metric="cosine",
        random_state=42
    )

    reduced_embeddings = umap_model.fit_transform(embeddings)
    return umap_model, reduced_embeddings


def clusterizar(docs: List[str]) -> HDBSCAN:
    """
    Cria um modelo HDBSCAN para clusteriza√ß√£o dos documentos com base na quantidade total.

    O tamanho m√≠nimo do cluster √© definido como 0,25% do total de documentos,
    com m√©trica euclidiana e m√©todo de sele√ß√£o de cluster 'eom'.

    Args:
        docs (List[str]): Lista de documentos validados.

    Returns:
        HDBSCAN: Modelo configurado para clusteriza√ß√£o hier√°rquica densa.
    """
    total_documentos = len(docs)
    percentual = 0.0025
    min_cluster_size = int(total_documentos * percentual)

    hdbscan_model = HDBSCAN(
        min_cluster_size=min_cluster_size,
        metric="euclidean",
        cluster_selection_method="eom",
        prediction_data=True
    )

    return hdbscan_model


def definir_prompt(coluna: str) -> Optional[str]:
    """
    Define o texto-base do prompt para gera√ß√£o de r√≥tulos de t√≥picos via LLM,
    de acordo com a coluna analisada.

    O texto retornado ser√° combinado internamente com palavras-chave e trechos
    representativos pelo modelo de representa√ß√£o.

    Args:
        coluna (str): Nome da coluna utilizada na an√°lise (ex: 'NormPredicacao', 'NormConclusao').

    Returns:
        Optional[str]: Texto-base do prompt correspondente ao tipo de coluna, ou None se n√£o reconhecida.
    """

    if coluna == 'NormPredicacao':
        return """
Estou fazendo um estudo sistem√°tico de discursos feitos por senadores no plen√°rio do Senado Federal.
Resolvi modelar os t√≥picos do meu banco de dados, conforme o que falam sobre a Constitui√ß√£o Federal.
Este t√≥pico diz respeito aos aspectos normativos da Constitui√ß√£o que s√£o citados nos discursos dos senadores.

Com base nas palavras-chave e exemplos fornecidos, extraia um r√≥tulo claro e espec√≠fico no seguinte formato:
topic: <r√≥tulo do t√≥pico>

Baseie o r√≥tulo nos aspectos normativos citados, e n√£o na interpreta√ß√£o deles.
Evite r√≥tulos vagos como "aspectos normativos da Constitui√ß√£o".
Voc√™ pode se basear nestes exemplos: 'Princ√≠pios e objetivos fundamentais', 'Separa√ß√£o de poderes', 'Direitos sociais'.
"""

    elif coluna == 'NormImplicacao':
        return """
Estou fazendo um estudo sistem√°tico de discursos feitos por senadores no plen√°rio do Senado Federal.
Este t√≥pico diz respeito √†s implica√ß√µes dos aspectos normativos da Constitui√ß√£o citados nos discursos.

Em outras palavras, trata-se da etapa entre a predica√ß√£o e a conclus√£o ‚Äî ou seja,
o que os oradores pensam a respeito dos princ√≠pios constitucionais mencionados.

Com base nas palavras-chave e exemplos fornecidos, extraia um r√≥tulo no formato:
topic: <r√≥tulo do t√≥pico>

Exemplos: 'Valoriza√ß√£o profissional', 'Defesa do meio ambiente'
"""

    elif coluna == 'NormConclusao':
        return """
Estou fazendo um estudo sistem√°tico de discursos feitos por senadores no plen√°rio do Senado Federal.
Este t√≥pico diz respeito √†s conclus√µes dos racioc√≠nios normativos baseados na Constitui√ß√£o.

Com base nas palavras-chave e nos exemplos de discurso, extraia um r√≥tulo curto no formato:
topic: <r√≥tulo do t√≥pico>

Exemplos: 'Pelo impeachment de presidente da Rep√∫blica', 'Contra impeachment de ministro do STF'
"""

    elif coluna == 'NormCombinado':
        return """
Estou fazendo um estudo sistem√°tico de discursos feitos por senadores no plen√°rio do Senado Federal.
Este t√≥pico diz respeito ao racioc√≠nio normativo completo ‚Äî desde a cita√ß√£o do aspecto constitucional
at√© a sua implica√ß√£o e eventual conclus√£o.

Com base nas palavras-chave e trechos fornecidos, extraia um r√≥tulo claro e conciso:
topic: <r√≥tulo do t√≥pico>
"""

    return None

def representacao() -> Tuple[Dict[str, object], SentenceTransformer]:
    """
    Cria o dicion√°rio de modelos de representa√ß√£o para o BERTopic,
    com base em an√°lise morfossint√°tica (POS) como representa√ß√£o principal.

    Returns:
        Tuple[Dict[str, object], SentenceTransformer]:
            - Dicion√°rio com representa√ß√µes ('Main', 'Aspect1')
            - Modelo de embeddings sem√¢nticos para o BERTopic
    """
    pos_model = PartOfSpeech("pt_core_news_lg", top_n_words=30)
    pos_model.pos = ["NOUN", "PROPN", "ADJ"]

    mmr_model = MaximalMarginalRelevance(diversity=0.5, top_n_words=30)

    representation_model = {
        "Main": pos_model,
        "Aspect1": mmr_model
    }

    embedding_model = SentenceTransformer("paraphrase-multilingual-MiniLM-L12-v2")

    return representation_model, embedding_model


def modelar(
    vectorizer_model: CountVectorizer,
    umap_model: UMAP,
    hdbscan_model: HDBSCAN,
    representation_model: Dict[str, Any],
    embedding_model: SentenceTransformer,
    docs_validados: List[str],
    reduced_embeddings: np.ndarray
) -> Tuple[BERTopic, List[int], np.ndarray]:
    """
    Cria e treina um modelo BERTopic completo com base em documentos e embeddings fornecidos.

    Integra os componentes: vetorizador, redu√ß√£o de dimensionalidade (UMAP), clusteriza√ß√£o (HDBSCAN),
    representa√ß√£o via LLM (OpenAI) e POS tagging, al√©m do modelo de embedding sem√¢ntico.

    Args:
        vectorizer_model (CountVectorizer): Vetorizador de texto.
        umap_model (UMAP): Modelo UMAP configurado para redu√ß√£o de dimensionalidade.
        hdbscan_model (HDBSCAN): Modelo de clusteriza√ß√£o HDBSCAN.
        representation_model (Dict[str, Any]): Dicion√°rio com modelos de representa√ß√£o sem√¢ntica.
        embedding_model (SentenceTransformer): Modelo de embedding sem√¢ntico.
        docs_validados (List[str]): Lista de documentos validados.
        reduced_embeddings (np.ndarray): Embeddings reduzidos com UMAP.

    Returns:
        Tuple[BERTopic, List[int], np.ndarray]:
            - Modelo BERTopic treinado
            - Lista de t√≥picos atribu√≠dos
            - Distribui√ß√£o de probabilidades por t√≥pico
    """
    topic_model = BERTopic(
        vectorizer_model=vectorizer_model,
        language="multilingual",
        umap_model=umap_model,
        hdbscan_model=hdbscan_model,
        representation_model=representation_model,
        embedding_model=embedding_model
    )

    topics, probs = topic_model.fit_transform(docs_validados, reduced_embeddings)

    return topic_model, topics, probs


def lista_topicos(topic_model: BERTopic) -> None:
    """
    Exibe a tabela de informa√ß√µes dos t√≥picos gerados pelo modelo BERTopic.

    Inclui n√∫mero do t√≥pico, quantidade de documentos e palavras representativas.

    Args:
        topic_model (BERTopic): Modelo de t√≥picos treinado.

    Returns:
        None
    """
    print(topic_model.get_topic_info())


def reduzir_topicos(
    topic_model: BERTopic,
    docs: list[str],
    nr_topics: Union[int, str] = "auto"
) -> None:
    """
    Reduz o n√∫mero de t√≥picos do modelo BERTopic com base em agrupamento sem√¢ntico.

    Essa redu√ß√£o pode ser autom√°tica ("auto") ou definida manualmente (com um n√∫mero inteiro).
    A fun√ß√£o informa a quantidade de t√≥picos antes e depois da opera√ß√£o.

    Args:
        topic_model (BERTopic): Modelo BERTopic j√° treinado.
        docs (list[str]): Lista de documentos originais.
        nr_topics (Union[int, str], optional): N√∫mero alvo de t√≥picos ou "auto". Default √© "auto".

    Returns:
        None
    """
    qtd_antes = len(topic_model.get_topic_info())
    topic_model.reduce_topics(docs, nr_topics)
    qtd_depois = len(topic_model.get_topic_info())

    print(f"T√≥picos reduzidos de {qtd_antes} para {qtd_depois}.")
    return topic_model


def salvar_resultados(
    topic_model: BERTopic,
    df: pd.DataFrame,
    docs: List[str],
    probs: np.ndarray,
    coluna: str,
    reduzido: bool = False
) -> None:
    """
    Salva os principais resultados da modelagem de t√≥picos, incluindo:
    - Lista de t√≥picos (get_topic_info)
    - Distribui√ß√£o de t√≥picos por documento
    - Evolu√ß√£o temporal dos t√≥picos

    Args:
        topic_model (BERTopic): Modelo BERTopic treinado.
        df (pd.DataFrame): DataFrame com os discursos v√°lidos.
        docs (List[str]): Lista de documentos utilizados na modelagem.
        probs (np.ndarray): Distribui√ß√£o de probabilidade por t√≥pico.
        coluna (str): Nome da coluna analisada.
        reduzido (bool, optional): Indica se os resultados s√£o da vers√£o reduzida. Default √© False.

    Returns:
        None
    """
    sufixo = "_reduzido" if reduzido else ""

    DIRETORIO = f"./{coluna}"
    if not os.path.exists(DIRETORIO):
        os.makedirs(DIRETORIO)

    # 1. Salvar info dos t√≥picos
    df_topicos = topic_model.get_topic_info()
    caminho_topicos = f"{DIRETORIO}/topicos_{coluna}{sufixo}.csv"
    df_topicos.to_csv(caminho_topicos, index=False, encoding="utf-8")

    # 2. Salvar t√≥picos por documento
    topicos = topic_model.topics_
    df_docs = df.copy()
    df_docs["Topico"] = topicos
    df_docs["Probs"] = probs.tolist() if probs is not None else None
    caminho_docs = f"{DIRETORIO}/documentos_topicos_{coluna}{sufixo}.csv"
    df_docs.to_csv(caminho_docs, index=False, encoding="utf-8")

    # 3. Salvar gr√°fico
    caminho_grafico = f"{DIRETORIO}/grafico_topicos_{coluna}{sufixo}.png"
    fig = topic_model.visualize_topics()
    fig.write_image(caminho_grafico, scale=2)

    # 4. Salvar r√≥tulos
    rotulos = topic_model.topic_aspects_.get("Aspect1", {})
    df_rotulos = pd.DataFrame([
        {
            "Topic": topic,
            "Rotulo": ", ".join([str(p[0]) for p in palavras if isinstance(p, (tuple, list)) and len(p) > 0])
        }
        for topic, palavras in rotulos.items()
    ])

    caminho_rotulos = f"{DIRETORIO}/rotulos_{coluna}{sufixo}.csv"
    df_rotulos.to_csv(caminho_rotulos, index=False, encoding="utf-8")

    # 5. Salvar evolu√ß√£o temporal
    timestamps = df["DataPronunciamento"].dt.year.tolist()
    topics_over_time = topic_model.topics_over_time(
        docs=docs,
        topics=topicos,
        timestamps=timestamps,
        global_tuning=True
    )
    df_tempo = pd.DataFrame(topics_over_time)
    caminho_tempo = f"{DIRETORIO}/topicos_{coluna}_tempo{sufixo}.csv"
    df_tempo.to_csv(caminho_tempo, index=False, encoding="utf-8")

    # 6. Salvar hierarquia de t√≥picos
    caminho_tree = f"{DIRETORIO}/arvore_topicos_{coluna}{sufixo}.txt"

    # Gera√ß√£o da estrutura hier√°rquica
    hierarchical_topics = topic_model.hierarchical_topics(docs)
    tree = topic_model.get_topic_tree(hierarchical_topics)

    # Redirecionar o print da √°rvore para um buffer de texto
    buffer = StringIO()
    print(tree, file=buffer)

    # Salvar o conte√∫do em arquivo
    with open(caminho_tree, "w", encoding="utf-8") as f:
        f.write(buffer.getvalue())


    print(f"\nüìÅ Resultados salvos com sucesso:")
    print(f"‚Üí {caminho_topicos}")
    print(f"‚Üí {caminho_docs}")
    print(f"‚Üí {caminho_tempo}")
    print(f"‚Üí {caminho_grafico}")
    print(f"‚Üí {caminho_rotulos}")
    print(f"‚Üí {caminho_tree}")




def main() -> None:
    """
    Executa o pipeline completo de modelagem de t√≥picos com BERTopic.
    Inclui carregamento, processamento, modelagem, redu√ß√£o, visualiza√ß√£o e exporta√ß√£o.
    """
    print("\nüì• In√≠cio do pipeline BERTopic\n")

    coluna = input("Digite a coluna de embeddings a ser analisada: ").strip()

    print("\n[1/5] Carregando dados e embeddings...")
    df_discursos = carregar_dados()
    codigos, embeddings = carregar_embeddings(coluna)
    df_valido, embeddings_validos = verificar_embeddings(codigos, embeddings, df_discursos)
    df_valido = formatar_datas(df_valido)
    salvar_dataset_embeddings(df_valido, embeddings_validos, coluna)

    print("\n[2/5] Preparando documentos, vetorizador e UMAP...")
    docs_validados = preparar_docs_bertopic(df_valido, coluna)
    vectorizer_model = treinar_vetorizador()
    umap_model, reduced_embeddings = reduzir_dim_embeddings(embeddings_validos)
    hdbscan_model = clusterizar(docs_validados)
    representation_model, embedding_model = representacao()

    print("\n[3/5] Modelando t√≥picos com BERTopic")
    topic_model, topics, probs = modelar(
        vectorizer_model,
        umap_model,
        hdbscan_model,
        representation_model,
        embedding_model,
        docs_validados,
        reduced_embeddings
    )

    print("\n[4/5] Visualiza√ß√£o antes da redu√ß√£o de t√≥picos:")
    lista_topicos(topic_model)
    salvar_resultados(topic_model, df_valido, docs_validados, probs, coluna)

    print("\n[5/5] Redu√ß√£o e visualiza√ß√£o final:")
    topic_model = reduzir_topicos(topic_model, docs_validados)
    lista_topicos(topic_model)
    salvar_resultados(topic_model, df_valido, docs_validados, probs, coluna, reduzido=True)

    print("\n‚úÖ Pipeline finalizado com sucesso!\n")


if __name__ == "__main__":
    main()









