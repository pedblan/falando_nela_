{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pedblan/falando_nela_v2/blob/main/5_modelagem_topicos_complementacao_v_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Configura√ß√£o ###"
      ],
      "metadata": {
        "id": "j1qK57fpilbl"
      },
      "id": "j1qK57fpilbl"
    },
    {
      "cell_type": "code",
      "id": "initial_id",
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "is_executing": true
        },
        "id": "initial_id"
      },
      "source": [
        "import os\n",
        "\n",
        "try:\n",
        "  import google.colab\n",
        "  IN_COLAB = True\n",
        "except:\n",
        "  IN_COLAB = False\n",
        "\n",
        "if IN_COLAB:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    os.chdir('/content/drive/MyDrive/falando_nela_v2/src')\n",
        "    print(\"Current working directory (Colab):\", os.getcwd())\n",
        "else:\n",
        "  print(\"Current working directory (not Colab):\", os.getcwd())"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Conex√£o ao banco de dados ###"
      ],
      "metadata": {
        "id": "e4Rga3YoiwAb"
      },
      "id": "e4Rga3YoiwAb"
    },
    {
      "metadata": {
        "id": "fe94006254dae95f"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null,
      "source": [
        "# prompt: Conecte-se a este banco de dados: /content/drive/MyDrive/falando_nela_v2/data/DiscursosSenadores.sqlite\n",
        "\n",
        "import sqlite3\n",
        "DB_PATH = '/content/drive/MyDrive/falando_nela_v2/data/DiscursosSenadores.sqlite'\n",
        "\n",
        "conn = sqlite3.connect(DB_PATH)\n",
        "cursor = conn.cursor()\n",
        "\n",
        "\n"
      ],
      "id": "fe94006254dae95f"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openpyxl\n",
        "\n",
        "import sqlite3\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "# Consulta de senadores\n",
        "query_senadores = \"\"\"\n",
        "SELECT\n",
        "    s.CodigoParlamentar,\n",
        "    s.NomeParlamentar,\n",
        "    s.SexoParlamentar,\n",
        "    s.NomeProfissao,\n",
        "    s.IndicadorAtividadePrincipal,\n",
        "    sc.NomeCargo,\n",
        "    sc.DataInicio AS DataInicioCargo,\n",
        "    sc.DataFim AS DataFimCargo,\n",
        "    sc.Orgao,\n",
        "    sha.NomeCurso,\n",
        "    sl.UnidadeLideranca,\n",
        "    sl.DescricaoTipoLideranca,\n",
        "    sl.DataInicio AS DataInicioLideranca,\n",
        "    sl.DataFim AS DataFimLideranca\n",
        "FROM Senadores s\n",
        "LEFT JOIN SenadoresCargos sc USING(CodigoParlamentar)\n",
        "LEFT JOIN SenadoresHistoricoAcademico sha USING(CodigoParlamentar)\n",
        "LEFT JOIN SenadoresLiderancas sl USING(CodigoParlamentar)\n",
        "\"\"\"\n",
        "\n",
        "# Consulta de discursos\n",
        "query_discursos = \"\"\"\n",
        "SELECT\n",
        "    d.CodigoPronunciamento,\n",
        "    d.CodigoParlamentar,\n",
        "    d.DataPronunciamento,\n",
        "    d.TextoResumo,\n",
        "    d.Indexacao,\n",
        "    d.SiglaPartidoParlamentarNaData,\n",
        "    d.UfParlamentarNaData,\n",
        "    d.SiglaCasaPronunciamento,\n",
        "    d.Forma,\n",
        "    d.TextoIntegral,\n",
        "    da.BM25_Constitui√ß√£o,\n",
        "    da.TFIDF_Constitui√ß√£o,\n",
        "    da.SentimentoGeral,\n",
        "    da.SentimentoConstituicao,\n",
        "    da.SumarioConstituicao,\n",
        "    da.TrechosConstituicao,\n",
        "    da.NovaConstituinteOuConstituicao_resposta,\n",
        "    da.NovaConstituinteOuConstituicao_trecho,\n",
        "    da.TopicosConstituicao,\n",
        "    dp.Similaridade_base_democracia,\n",
        "    dp.Similaridade_ultrapassada,\n",
        "    dp.Similaridade_prejudica_economia,\n",
        "    dp.Similaridade_direitos_demais,\n",
        "    dp.Similaridade_nova_constituinte,\n",
        "    dp.Similaridade_ffaa_poder_moderador,\n",
        "    dp.Similaridade_voltar_ditadura,\n",
        "    dp.Similaridade_governo_nao_respeita,\n",
        "    dp.Similaridade_camara_nao_respeita,\n",
        "    dp.Similaridade_supremo_nao_respeita,\n",
        "    dp.Similaridade_ninguem_respeita,\n",
        "    dn.MencionaConstituicao,\n",
        "    dn.NormPredicacao,\n",
        "    dn.NormImplicacao,\n",
        "    dn.NormConclusao,\n",
        "    dn.NormTrecho,\n",
        "    dn.AvalPredicacao,\n",
        "    dn.AvalImplicacao,\n",
        "    dn.AvalConclusao,\n",
        "    dn.AvalTrecho\n",
        "FROM Discursos d\n",
        "LEFT JOIN DiscursosAnalises da USING(CodigoPronunciamento)\n",
        "LEFT JOIN DiscursosPesquisa dp USING(CodigoPronunciamento)\n",
        "LEFT JOIN DiscursosNarrativas dn USING(CodigoPronunciamento)\n",
        "\"\"\"\n",
        "\n",
        "# L√™ os dados em DataFrames\n",
        "df_senadores = pd.read_sql_query(query_senadores, conn)\n",
        "df_discursos = pd.read_sql_query(query_discursos, conn)\n",
        "\n",
        "# Fecha conex√£o\n",
        "conn.close()\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "R4tT9UE4fgz3"
      },
      "id": "R4tT9UE4fgz3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Modelagem de t√≥picos ###"
      ],
      "metadata": {
        "id": "RTBZK4hYpEAM"
      },
      "id": "RTBZK4hYpEAM"
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "api_key = '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'\n",
        "client = OpenAI(api_key=api_key)\n"
      ],
      "metadata": {
        "id": "vR5Lb0wzo_VR"
      },
      "id": "vR5Lb0wzo_VR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instala√ß√µes necess√°rias (para Colab)\n",
        "!pip install bertopic umap-learn hdbscan tiktoken openai nltk\n",
        "\n",
        "# IMPORTA√á√ïES\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "from sklearn.preprocessing import normalize\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "import umap\n",
        "import hdbscan\n",
        "import tiktoken\n",
        "import openai\n",
        "from bertopic import BERTopic\n",
        "from bertopic.representation import OpenAI\n",
        "from sklearn.preprocessing import normalize\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "xqGZSZz6xO6b"
      },
      "id": "xqGZSZz6xO6b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Baixar stopwords do NLTK (se necess√°rio)\n",
        "import nltk\n",
        "nltk.download(\"stopwords\")\n",
        "\n",
        "# Criar lista de stopwords combinada (NLTK + personalizadas)\n",
        "stopwords_pt = stopwords.words(\"portuguese\")"
      ],
      "metadata": {
        "id": "iDmIgn78zgU7"
      },
      "id": "iDmIgn78zgU7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summarization_prompt_predicacao = \"\"\"\n",
        "Voc√™ est√° analisando clusters que representam **afirma√ß√µes normativas ou descritivas** em discursos parlamentares ‚Äî aquilo que √© dito sobre a Constitui√ß√£o.\n",
        "\n",
        "Palavras-chave do cluster:\n",
        "[KEYWORDS]\n",
        "\n",
        "Amostra representativa:\n",
        "[DOCUMENTS]\n",
        "\n",
        "Baseando-se nisso, forne√ßa um **r√≥tulo tem√°tico claro e expressivo** no formato:\n",
        "topic: <t√≥pico>\n",
        "\n",
        "üîπ Use linguagem expressiva.\n",
        "üîπ Seja espec√≠fico e escreva em portugu√™s. N√£o √© necess√°rio escrever uma frase completa, do tipo 'este t√≥pico diz que...'.\n",
        "\n",
        "Exemplos de sa√≠da:\n",
        "['Constitui√ß√£o de 1988 como marco democr√°tico e de direitos no Brasil.']\n",
        "['Desequil√≠brio na distribui√ß√£o de recursos e autonomia dos entes federativos segundo a Constitui√ß√£o de 1988']\n",
        "['Falhas e lacunas na Constitui√ß√£o relacionadas ao foro privilegiado e impunidade']\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "4rn0lkrx0S-s"
      },
      "id": "4rn0lkrx0S-s",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summarization_prompt_implicacao = \"\"\"\n",
        "Voc√™ est√° analisando clusters que expressam **implica√ß√µes ou consequ√™ncias normativas** ‚Äî o que os discursos deduzem como resultado l√≥gico do que afirmam.\n",
        "\n",
        "Palavras-chave do cluster:\n",
        "[KEYWORDS]\n",
        "\n",
        "Amostra representativa:\n",
        "[DOCUMENTS]\n",
        "\n",
        "Forne√ßa um t√≥pico que resuma **a implica√ß√£o principal** do cluster:\n",
        "topic: <t√≥pico>\n",
        "\n",
        "üîπ Use linguagem expressiva.\n",
        "üîπ Seja espec√≠fico e escreva em portugu√™s. N√£o √© necess√°rio escrever uma frase completa, do tipo 'este t√≥pico diz que...'.\n",
        "\n",
        "Exemplos de sa√≠da:\n",
        "['Dificuldades financeiras dos munic√≠pios em cumprir responsabilidades devido √† falta de recursos']\n",
        "['Combate √† discrimina√ß√£o e promo√ß√£o da igualdade racial']\n",
        "['Igualdade de g√™nero na representa√ß√£o pol√≠tica']\n",
        "\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "2l-Hbqpl0THw"
      },
      "id": "2l-Hbqpl0THw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summarization_prompt_conclusao = \"\"\"\n",
        "Voc√™ est√° analisando clusters que expressam **conclus√µes pol√≠ticas ou normativas** ‚Äî o que o discurso prop√µe fazer ou mudar com base nos argumentos anteriores.\n",
        "\n",
        "Palavras-chave do cluster:\n",
        "[KEYWORDS]\n",
        "\n",
        "Amostra representativa:\n",
        "[DOCUMENTS]\n",
        "\n",
        "Indique o **objetivo ou proposta** que emerge do cluster:\n",
        "topic: <t√≥pico>\n",
        "\n",
        "üîπ Use linguagem expressiva.\n",
        "üîπ Seja espec√≠fico e escreva em portugu√™s. N√£o √© necess√°rio escrever uma frase completa, do tipo 'este t√≥pico diz que...'.\n",
        "\n",
        "Exemplos de sa√≠da:\n",
        "['Reforma pol√≠tica atrav√©s de Assembleia Constituinte exclusiva']\n",
        "['Regulamenta√ß√£o e financiamento da sa√∫de p√∫blica atrav√©s da Emenda Constitucional n¬∫ 29']\n",
        "['Fim do foro privilegiado']\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "AGj_Jw-Z0aPp"
      },
      "id": "AGj_Jw-Z0aPp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "client = openai.OpenAI(api_key=api_key)\n",
        "tokenizer = tiktoken.encoding_for_model(\"gpt-4o-2024-08-06\")\n",
        "\n",
        "def representacao(tipo_de_analise):\n",
        "  return representation_model = OpenAI(\n",
        "      client=client,\n",
        "      model=\"gpt-4o-2024-08-06\",\n",
        "      prompt=tipo_de_analise,\n",
        "      delay_in_seconds=2,\n",
        "      chat=True,\n",
        "      nr_docs=10,\n",
        "      doc_length=200,\n",
        "      tokenizer=tokenizer\n",
        "  )\n",
        "\n",
        "# Criar modelo UMAP customizado\n",
        "umap_model = umap.UMAP(\n",
        "    n_neighbors=15,\n",
        "    n_components=5,\n",
        "    min_dist=0.0,\n",
        "    metric=\"cosine\",\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Criar modelo HDBSCAN customizado\n",
        "hdbscan_model = hdbscan.HDBSCAN(\n",
        "    min_cluster_size=10,\n",
        "    metric=\"euclidean\",\n",
        "    cluster_selection_method=\"eom\",\n",
        "    prediction_data=True\n",
        ")\n",
        "\n",
        "# Criar vetor de contagem com todas as stopwords\n",
        "vectorizer_model = CountVectorizer(stop_words=stopwords_pt)"
      ],
      "metadata": {
        "id": "brQP5kthx4Xn"
      },
      "id": "brQP5kthx4Xn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install faiss-cpu\n",
        "\n",
        "import os\n",
        "import faiss\n",
        "import pickle\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from bertopic import BERTopic\n",
        "from sklearn.preprocessing import normalize\n",
        "\n",
        "# Fun√ß√£o para salvar embeddings em FAISS\n",
        "def salvar_embeddings_faiss(embeddings, codigos, nome_base):\n",
        "    dimension = len(embeddings[0])\n",
        "    index = faiss.IndexFlatL2(dimension)\n",
        "    index.add(np.array(embeddings).astype('float32'))\n",
        "\n",
        "    faiss.write_index(index, f\"../data/modelos_bertopic/{nome_base}_faiss.index\")\n",
        "\n",
        "    with open(f\"../data/modelos_bertopic/{nome_base}_metadados.pkl\", \"wb\") as f:\n",
        "        pickle.dump(codigos, f)\n",
        "\n",
        "    print(f\"[‚úî] Embeddings salvos em FAISS para {nome_base}\")\n",
        "\n",
        "# Fun√ß√£o para carregar embeddings do FAISS (se existir)\n",
        "def carregar_embeddings_faiss(nome_base):\n",
        "    index_path = f\"../data/modelos_bertopic/{nome_base}_faiss.index\"\n",
        "    meta_path = f\"../data/modelos_bertopic/{nome_base}_metadados.pkl\"\n",
        "\n",
        "    if os.path.exists(index_path) and os.path.exists(meta_path):\n",
        "        index = faiss.read_index(index_path)\n",
        "        with open(meta_path, \"rb\") as f:\n",
        "            codigos = pickle.load(f)\n",
        "        embeddings = index.reconstruct_n(0, index.ntotal)\n",
        "        print(f\"[üîÅ] Embeddings carregados de FAISS para {nome_base}\")\n",
        "        return normalize(np.array(embeddings)), codigos\n",
        "    else:\n",
        "        return None, None\n",
        "\n",
        "# Fun√ß√£o para gerar embeddings com OpenAI\n",
        "def gerar_embeddings_openai(textos, client, model=\"text-embedding-3-large\"):\n",
        "    embeddings = []\n",
        "    for texto in tqdm(textos, desc=\"Gerando embeddings\"):\n",
        "        try:\n",
        "            response = client.embeddings.create(input=texto, model=model)\n",
        "            embeddings.append(response.data[0].embedding)\n",
        "        except Exception as e:\n",
        "            print(f\"Erro ao gerar embedding: {e}\")\n",
        "            embeddings.append([0.0] * 1536)\n",
        "    return normalize(np.array(embeddings))\n",
        "\n"
      ],
      "metadata": {
        "id": "0i4ahgz_uwVP"
      },
      "id": "0i4ahgz_uwVP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Listagem das colunas\n",
        "colunas_para_modelar = [\n",
        "    'NormPredicacao', 'NormImplicacao', 'NormConclusao',\n",
        "    'AvalPredicacao', 'AvalImplicacao', 'AvalConclusao'\n",
        "]\n",
        "\n",
        "# Cria√ß√£o das pastas\n",
        "os.makedirs(\"../data/modelos_bertopic\", exist_ok=True)\n",
        "os.makedirs(\"../data/topicos_gerados/v2\", exist_ok=True)\n",
        "\n",
        "dfs_topicos = {}\n",
        "\n",
        "for coluna in colunas_para_modelar:\n",
        "    print(f\"\\nüîç Modelando t√≥picos para: {coluna}\")\n",
        "\n",
        "    df_filtrado = df_discursos[df_discursos[coluna].notna() & df_discursos[coluna].str.strip().astype(bool)]\n",
        "    textos = df_filtrado[coluna].tolist()\n",
        "    codigos = df_filtrado[\"CodigoPronunciamento\"].tolist()\n",
        "\n",
        "    # Tenta carregar os embeddings do FAISS\n",
        "    embeddings_normalizados, codigos_salvos = carregar_embeddings_faiss(coluna)\n",
        "\n",
        "    # Se n√£o houver, gera e salva\n",
        "    if embeddings_normalizados is None:\n",
        "        embeddings_normalizados = gerar_embeddings_openai(textos, client)\n",
        "        salvar_embeddings_faiss(embeddings_normalizados, codigos, nome_base=coluna)\n",
        "    else:\n",
        "        # Verifica se os c√≥digos s√£o compat√≠veis\n",
        "        if codigos != codigos_salvos:\n",
        "            print(f\"[‚ö†Ô∏è] Aten√ß√£o: os c√≥digos atuais diferem dos salvos! Recalculando.\")\n",
        "            embeddings_normalizados = gerar_embeddings_openai(textos, client)\n",
        "            salvar_embeddings_faiss(embeddings_normalizados, codigos, nome_base=coluna)\n",
        "\n",
        "    if coluna == 'NormPredicacao' or 'AvalPredicacao':\n",
        "        representation_model = representacao(summarization_prompt_predicacao)\n",
        "    elif coluna == 'NormImplicacao' or 'AvalImplicacao':\n",
        "        representation_model = representacao(summarization_prompt_implicacao)\n",
        "    elif coluna == 'NormConclusao' or 'AvalConclusao':\n",
        "        representation_model = representacao(summarization_prompt_conclusao)\n",
        "\n",
        "    # Cria√ß√£o do modelo\n",
        "    modelo_clone = BERTopic(\n",
        "        embedding_model=None,\n",
        "        umap_model=umap_model,\n",
        "        hdbscan_model=hdbscan_model,\n",
        "        vectorizer_model=vectorizer_model,\n",
        "        representation_model=representation_model,\n",
        "        language=\"multilingual\",\n",
        "        verbose=True\n",
        "    )\n",
        "\n",
        "    topics, probs = modelo_clone.fit_transform(textos, embeddings=embeddings_normalizados)\n",
        "\n",
        "    df_topicos = modelo_clone.get_document_info(textos)\n",
        "    df_topicos[\"CodigoPronunciamento\"] = codigos\n",
        "\n",
        "    dfs_topicos[f\"{coluna}_topicos\"] = df_topicos\n",
        "    df_topicos.to_csv(f\"../data/topicos_gerados/v_2{coluna}_topicos.csv\", index=False)\n",
        "\n",
        "    print(modelo_clone.get_topic_info().head())\n",
        "\n",
        "    fig = modelo_clone.visualize_hierarchy()\n",
        "    fig.write_html(f\"hierarquia_topicos_{coluna}.html\")\n"
      ],
      "metadata": {
        "id": "tZnOrrLNzNMw"
      },
      "id": "tZnOrrLNzNMw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Atualiza√ß√£o do banco de dados ###"
      ],
      "metadata": {
        "id": "7AYt1Z2TvIva"
      },
      "id": "7AYt1Z2TvIva"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Produzir xlsx para an√°lise no Tableau ### ACRESCENTAR T√ìPICOS AO DF_DISCURSOS"
      ],
      "metadata": {
        "id": "jZptiz31qK4_"
      },
      "id": "jZptiz31qK4_"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uie7Xm8bzljY"
      },
      "id": "uie7Xm8bzljY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# Fun√ß√£o para remover caracteres ilegais para Excel (control chars)\n",
        "def limpar_caracteres_invalidos(df):\n",
        "    def limpar_valor(val):\n",
        "        if isinstance(val, str):\n",
        "            # Remove caracteres invis√≠veis/ilegais\n",
        "            return re.sub(r\"[\\x00-\\x08\\x0B\\x0C\\x0E-\\x1F\\x7F-\\x9F]\", \"\", val)\n",
        "        return val\n",
        "    return df.applymap(limpar_valor)\n",
        "\n",
        "# Aplica a limpeza antes de salvar\n",
        "df_senadores_limpo = limpar_caracteres_invalidos(df_senadores)\n",
        "df_discursos_limpo = limpar_caracteres_invalidos(df_discursos)\n",
        "\n",
        "# Salva no Excel com openpyxl\n",
        "with pd.ExcelWriter(\"../data/dados_para_tableau_v_2.xlsx\", engine=\"openpyxl\") as writer:\n",
        "    df_senadores_limpo.to_excel(writer, sheet_name=\"Senadores\", index=False)\n",
        "    df_discursos_limpo.to_excel(writer, sheet_name=\"Discursos\", index=False)\n",
        "\n",
        "print(\"[‚úÖ] Arquivo Excel salvo com sucesso ap√≥s limpar caracteres ilegais!\")\n",
        "\n"
      ],
      "metadata": {
        "id": "daXZbBX5KsCp"
      },
      "id": "daXZbBX5KsCp",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}